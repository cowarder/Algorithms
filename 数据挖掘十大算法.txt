决策树学习算法包括：特征选择、决策树生成、决策树剪枝
决策树生成考虑的是局部特征，决策树剪枝考虑的是全局特征
熵：表示随机变量不确定性的度量，数据越混乱，熵越大
条件熵H(Y|X)：表示在已知随机变量X的条件下随机变量Y的不确定性，求和(pi*H(Y|X=xi))
信息增益：熵-条件熵    熵H(D)表示对数据集D进行分类的不确定性，条件熵H(D|A)表示在给定特征条件A下
		对数据集分类的不确定性，他们之间的差表示特征A使得数据集D不确定性下降的程度，越大具有越
		好的分类能力

（1）C4.5
输入：训练数据集D，特征集A，阈值thresh
输出：决策树T

1.如果D中所有数据属于同一类C，置T为单节点树，将C作为该节点的类
2.如果A为空集（即没有特征可以分割），那么置T为单节点树，将D中数目最多的类C作为该节点的类
3.计算A中所有特征的信息增益比，选择增益比最大的特征a
4.如果a的信息增益比小于阈值thresh，置T为单节点树，将D中数目最多的类C作为该节点的类
5.利用特征a将D划分为若干子集，构建子节点，每个子集中数目最多的类标记为该节点的类
6.对每个子节点，去掉特征a，重复上述步骤1-5

解释：C4.5与ID3主要的区别在于，ID3选择特征的策略采用的是信息增益，但是C4.5采用信息增益比


（2）k-means
k均值算法主要包括两个步骤：1.数据的分配（确定它是属于哪一个聚类）  2.聚类中心的重定位
k均值算法存在的三个问题：1.k值的选择
						 2.容易受到初始化聚类中心的影响，陷入局部最优	
						 3.容易受到异常值的影响
						 4.对于非凸非球型的数据，k均值可能不能很好的描述中心位置
						 
						 
（3）SVM

支持向量机由繁至简分为三种：1.线性可分支持向量机与硬间隔最大化 
							2.线性支持向量机与软间隔最大化
							3.非线性支持向量机与核函数

线性可分支持向量机：
	以二分类为例，可以找到一个分隔超平面(wx+b=0)将数据分隔开来，这样的超平面有无数多个
	类标号：f(x)=sign(wx+b)
	
	函数间隔与几何间隔：
		一般来说，距离分隔超平面距离越近，说明它分类正确的置信度越小，距离分隔超平面越远
		说明它分类正确的置信度越大，就需要一个度量标准来衡量不同数据到分隔超平面的距离
		
		函数间隔：dis = y(wx+b) x表示某个数据点
		几何间隔：dis=y(wx+b)/||w||  这是因为如果成比例改变w和b，分隔超平面不变，但是函数间隔却成比例改变
				，这就导致了分隔超平面的不确定性，所以需要对间隔进行归一化，如果超平面参数w和b成比例增加，
				函数间隔会增加，但是几何间隔不会改变
			
	支持向量机的想法是求解能够正确划分训练数据集并且几何间隔最大的分隔超平面
	对于线性可分的数据集来说，线性可分隔超平面有无数多个，但是几何间隔最大的分隔超平面是唯一的
	
	分隔超平面不仅能够将正负实例分隔，而且使得距离超平面最近的点到超平面距离尽可能大
	
	约束最优化推导：
		dis：几何间隔
		max dis
		s.t.  y(wx+b)/||w||>=dis
		根据：几何间隔=函数间隔/||w||，上述优化过程可以改写为：
		
		max dis/||w||
		s.t. y(wx+b)>=dis
		其中dis代表函数间隔
		
		但是函数间隔不会影响最优化问题的解，因为不等式两端是等比例变化的，所以，限定函数间隔为1进行优化
		这就得出了线性可分支持向量机的优化原理：
		min ||w||^2/2
		s.t. y(wx+b)-1>=0
		即为一个凸二次优化的过程
		
	总结线性可分支持向量机的优化过程：
		1.构造并解约束最优化问题
		2.得到分隔超平面，得到决策函数f(x)=sign(wx+b)
	
	在间隔边界上面的点称为支持向量，在决定分隔超平面的时候仅有支持向量起作用，其他实例点不起作用
	
	怎样去解约束最优化问题：
		应用拉格朗日对偶算法，通过求解对偶问题的最优解得到原始问题的最优解
		构建拉格朗日函数，为每一个不等式约束添加拉格朗日乘子
		
		为什么要引入对偶问题的求解？
			1.往往对偶问题更加容易求解
			2.自然引入核函数，推广到非线性分类器
	
	当对具有约束条件的问题进行优化，求解最优解的时候，需要满足KTT条件
		
		
		
线性支持向量机与软间隔最大化：
	当某些点不能满足函数间隔大于等于1的条件，就可以采用软间隔最大化
	就是在约束条件上面加上一个松弛项，y(wx+b)>=1-松弛项
	优化的目标函数变为||w||^2/2+C求和(松弛项)：有两层含义：1.最小化前项即最大化间隔    2.使得误分类的点的数量尽量少

非线性支持向量机与核函数：
	核函数将样本从一个低维空间映射到高维空间
	在优化的函数中，有一个项是向量之间的乘积，就可以用一个核函数来表示两个向量分别进行映射之后的乘积
	

SVM（支持向量机）：使得距离分隔超平面最近的样本点距离最大
SVC（支持向量回归）：使得距离到超平面最远的样本点距离最小
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	