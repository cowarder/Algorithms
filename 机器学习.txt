机器学习效果不好的解决方法：
1.数据
2.模型
3.参数
4.模型融合


核函数：
处理非线性问题，没有核函数和用核函数的区别：
1.一个是映射到高维空间中，然后再根据内积的公式进行计算；
2.而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。

核函数避免了直接在高维上面的直接计算


预剪枝方法(较早的停止树的生长)：
(1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。
(2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。
(3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。
(4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。


L1正则化与L2正则化的区别：
https://blog.csdn.net/jinping_shi/article/details/52433975

参数值越小往往以为着模型越简单
L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。
这两种方式的选择取决于在模型中是所有的特征都起作用还是仅有部分特征起作用，往往来说L2的效果更好，
因为在我们研究的问题中，提取到的特征往往是有意义的

深度学习中防止过拟合问题：
1.early_stopping
2.扩大数据集
3.正则化
4.drop_out
5.batchnorm

BN（均值为0，方差为1）
batch normalizaiton解决了反向传播中的梯度消失和梯度爆炸问题，使得不同scale的w整体更新步调一致

利用逻辑回归做非线性分类：
加kernel

激活函数的作用是什么？
激活函数是用来添加非线性因素的，因为线性模型的表达能力不够
使用激活函数能够使得这些原本线性不可分的数据变得线性可分

